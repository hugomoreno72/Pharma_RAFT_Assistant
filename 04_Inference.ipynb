{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gBLJ1UBLuyZ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth langchain langchain-huggingface faiss-cpu langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "# I unzip the zip file containing the model\n",
        "shutil.unpack_archive(\"lora_pharma.zip\", \"lora_pharma\", \"zip\")"
      ],
      "metadata": {
        "id": "3g5rhWgZcniB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "2iUBxJ3sc3oV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I load the model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"lora_pharma\",\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        ")"
      ],
      "metadata": {
        "id": "6eoFdn5mc5zE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)"
      ],
      "metadata": {
        "id": "AefXPORfL2r0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I load the vector database (FAISS)\n",
        "embeddings = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "vector_db = FAISS.load_local(\"faiss_pharmacy\", embeddings, allow_dangerous_deserialization=True)"
      ],
      "metadata": {
        "id": "HQGYY6eRc8CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pharmaceutical_question(question, k=5, critical_threshold=1.5):\n",
        "\n",
        "    # Retrieval\n",
        "    docs_with_score = vector_db.similarity_search_with_score(question, k=k)\n",
        "\n",
        "    # I extract the best score (the one of the first document)\n",
        "    best_score = docs_with_score[0][1]\n",
        "\n",
        "    # Validation of the threshold\n",
        "    if best_score > critical_threshold:\n",
        "        return \"The question is out of my domain, and I cannot find any related information.\", []\n",
        "\n",
        "    # If it passes the filter, I prepare the context\n",
        "    docs = [doc for doc, score in docs_with_score]\n",
        "    retrieved_context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "    # Prompt Preparation\n",
        "    messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a precise pharmaceutical assistant. If the answer is not in the context, say that you do not know, do not invent.\"},\n",
        "    {\"role\": \"user\", \"content\": f\"Context: {retrieved_context}\\n\\nQuestion: {question}\"},\n",
        "    ]\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(text=prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    #  Generation\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        use_cache=True,\n",
        "        temperature=0.1\n",
        "    )\n",
        "\n",
        "    answer = tokenizer.batch_decode(outputs)[0].split(\"<start_of_turn>model\\n\")[-1]\n",
        "    return answer.replace(\"<end_of_turn>\", \"\").replace(\"<eos>\", \"\").strip(), docs\n",
        "\n",
        "print(\"Pipeline configured\")"
      ],
      "metadata": {
        "id": "7E0DgO9fL5jq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a7580f0-4e11-4ac7-dce1-a87fcab02f72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline configured\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = \"What is the composition of Aciloc 150?\"\n",
        "\n",
        "answer, sources = pharmaceutical_question(user_question)\n",
        "\n",
        "print(f\"\\n--- QUESTION ---\\n{user_question}\")\n",
        "print(f\"\\n--- ASSISTANT'S RESPONSE ---\\n{answer}\")\n",
        "print(\"\\n--- SOURCES ---\")\n",
        "for i, doc in enumerate(sources):\n",
        "    print(f\"Fuente {i+1}: {doc.metadata.get('source', 'Unknown')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGbj7PpenB5x",
        "outputId": "41e1cdf6-f82a-4444-e132-56714b13d80e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- QUESTION ---\n",
            "What is the composition of Aciloc 150?\n",
            "\n",
            "--- ASSISTANT'S RESPONSE ---\n",
            "The composition of Aciloc 150 is Ranitidine (150mg).\n",
            "\n",
            "--- SOURCES ---\n",
            "Fuente 1: Kaggle_Dataset\n",
            "Fuente 2: Kaggle_Dataset\n",
            "Fuente 3: Kaggle_Dataset\n",
            "Fuente 4: Kaggle_Dataset\n",
            "Fuente 5: Kaggle_Dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = \"What are the contraindications of Azithromycin?\"\n",
        "\n",
        "answer, sources = pharmaceutical_question(user_question)\n",
        "\n",
        "print(f\"\\n--- QUESTION ---\\n{user_question}\")\n",
        "print(f\"\\n--- ASSISTANT'S RESPONSE ---\\n{answer}\")\n",
        "print(\"\\n--- SOURCES ---\")\n",
        "for i, doc in enumerate(sources):\n",
        "    print(f\"Fuente {i+1}: {doc.metadata.get('source', 'Unknown')}\")"
      ],
      "metadata": {
        "id": "Z5s03QyfL9BG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f08438f4-9c96-4a46-f874-f238876c0848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- QUESTION ---\n",
            "What are the contraindications of Azithromycin?\n",
            "\n",
            "--- ASSISTANT'S RESPONSE ---\n",
            "The provided context does not contain information about the contraindications of Azithromycin. It only mentions the uses and side effects of Azithromycin in the context of Azithral 200 Liquid and ATM 200 Oral Suspension, which are both formulations of Azithromycin. Therefore, I cannot answer the question about the contraindications of Azithromycin based on the given context.\n",
            "\n",
            "--- SOURCES ---\n",
            "Fuente 1: Katzung - Basic and Clinical Pharmacology 12th Edition (2012).pdf\n",
            "Fuente 2: Kaggle_Dataset\n",
            "Fuente 3: Kaggle_Dataset\n",
            "Fuente 4: Kaggle_Dataset\n",
            "Fuente 5: Kaggle_Dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = \"Who is Messi?\"\n",
        "\n",
        "answer, sources = pharmaceutical_question(user_question)\n",
        "\n",
        "print(f\"\\n--- QUESTION ---\\n{user_question}\")\n",
        "print(f\"\\n--- ASSISTANT'S RESPONSE ---\\n{answer}\")\n",
        "print(\"\\n--- SOURCES ---\")\n",
        "for i, doc in enumerate(sources):\n",
        "    print(f\"Fuente {i+1}: {doc.metadata.get('source', 'Unknown')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wa5MicUzi4Eb",
        "outputId": "70258560-7f6a-4ba5-ff6b-8a15b9864afc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- QUESTION ---\n",
            "Who is Messi?\n",
            "\n",
            "--- ASSISTANT'S RESPONSE ---\n",
            "The question is out of my domain, and I cannot find any related information.\n",
            "\n",
            "--- SOURCES ---\n"
          ]
        }
      ]
    }
  ]
}